# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YAXyXhnNq9xeVA-jgU5jRhRMxFzW_BP3

**How to preprocess and prepare the data for sentiment analysis?**
"""

# Import necessary libraries
import os
import pandas as pd
from sklearn.model_selection import train_test_split
import cchardet

# Load Arabic Twitter dataset
def load_dataset(data_path):
    texts = []
    labels = []
    for label in ["positive", "negative"]:
        folder_path = os.path.join(data_path, label)
        for filename in os.listdir(folder_path):
            with open(os.path.join(folder_path, filename), "rb") as file:
                rawdata = file.read()
                result = cchardet.detect(rawdata)
                encoding = result['encoding']
            with open(os.path.join(folder_path, filename), "r", encoding=encoding) as file:
                text = file.read()
                texts.append(text)
                labels.append(1 if label == "positive" else 0)
    return pd.DataFrame({"text": texts, "label": labels})

# Set path to your dataset
data_path = "/content/drive/MyDrive/Colab Notebooks/Twitter"
df = load_dataset(data_path)

# Tokenize and split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df["text"], df["label"], test_size=0.2, random_state=42)

"""**How to load the CAMeLBERT-CA SA model for sentiment analysis?**"""

# Import necessary libraries
from transformers import pipeline

# Load the CAMeLBERT-CA SA model
sa_model_name = "CAMeL-Lab/bert-base-arabic-camelbert-ca-sentiment"
sa = pipeline('text-classification', model=sa_model_name)

"""**How to tokenize and encode the text data for input to the CAMeLBERT-CA SA model?**"""

# Import necessary libraries
from transformers import AutoTokenizer

# Tokenize and encode the text data
tokenizer = AutoTokenizer.from_pretrained(sa_model_name)
X_train_encoded = tokenizer(X_train.tolist(), truncation=True, padding=True, return_tensors="pt")
X_test_encoded = tokenizer(X_test.tolist(), truncation=True, padding=True, return_tensors="pt")

"""**How to make predictions using the CAMeLBERT-CA SA model?**"""

# Make predictions using the CAMeLBERT-CA SA model
predictions = sa(df["text"].tolist())

"""**How to evaluate the performance of the sentiment analysis model?**"""

# Import necessary libraries
from sklearn.metrics import accuracy_score, classification_report

# Evaluate the performance of the model
y_pred = [1 if pred["label"] == 'positive' else 0 for pred in predictions]
accuracy = accuracy_score(df["label"], y_pred)
report = classification_report(df["label"], y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:\n", report)

"""**How to interpret the predictions made by the CAMeLBERT-CA SA model?**"""

# Interpret the predictions made by the CAMeLBERT-CA SA model
for text, prediction in zip(df["text"], predictions):
    sentiment_label = prediction["label"]
    sentiment_score = prediction["score"]
    print(f"Text: {text}\nPredicted Sentiment: {sentiment_label} (Score: {sentiment_score:.4f})\n")

"""**How to visualize the distribution of predicted sentiments?**"""

# Import necessary libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Visualize the distribution of predicted sentiments
sns.countplot(x=y_pred)
plt.xlabel('Predicted Sentiment')
plt.ylabel('Count')
plt.title('Distribution of Predicted Sentiments')
plt.show()

"""**How to save and load the CAMeLBERT-CA SA model for future use?**"""

# Save the CAMeLBERT-CA SA model
sa.save_pretrained("path/to/save/directory")

# Load the saved CAMeLBERT-CA SA model
loaded_sa = pipeline('text-classification', model="path/to/save/directory")

"""**How to handle imbalanced classes in the sentiment analysis dataset?**"""

# Import necessary libraries
from sklearn.utils import resample
from sklearn.model_selection import train_test_split  # Don't forget to import train_test_split

# Check class distribution
class_distribution = df["label"].value_counts()

# Handle imbalanced classes (if needed)
if len(class_distribution) > 1:
    # Resample the minority class to balance the dataset
    minority_class = df[df["label"] == class_distribution.idxmin()]
    majority_class = df[df["label"] == class_distribution.idxmax()]
    minority_class_resampled = resample(minority_class, replace=True, n_samples=class_distribution.idxmax())
    df_balanced = pd.concat([majority_class, minority_class_resampled])

    # Split the balanced dataset into training and testing sets
    X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(
        df_balanced["text"], df_balanced["label"], test_size=0.2, random_state=42)

"""**How to extract contextual embeddings using the CAMeLBERT-CA model?**"""

# Import necessary libraries
from transformers import AutoModel, AutoTokenizer

# Load CAMeLBERT-CA model and tokenizer
camelbert_ca_model_name = "CAMeL-Lab/bert-base-arabic-camelbert-ca"
tokenizer = AutoTokenizer.from_pretrained(camelbert_ca_model_name)
camelbert_ca_model = AutoModel.from_pretrained(camelbert_ca_model_name)

# Tokenize and encode text
text = "أهلاً بك في OpenAI"
tokens = tokenizer(text, return_tensors="pt")
embeddings = camelbert_ca_model(**tokens).last_hidden_state

"""**What are some examples of positive and negative tweets from the dataset?**"""

# Display examples of positive tweets
positive_samples = df[df['label'] == 1].sample(5)['text']
print("Examples of Positive Tweets:")
print(positive_samples)

# Display examples of negative tweets
negative_samples = df[df['label'] == 0].sample(5)['text']
print("\nExamples of Negative Tweets:")
print(negative_samples)

"""**What are the most frequent n-grams (bi-grams or tri-grams) in the dataset?**"""

# Import necessary libraries
from sklearn.feature_extraction.text import CountVectorizer

# Function to get most frequent n-grams
def get_top_ngrams(corpus, ngram_range=(2, 2), top_k=10):
    vec = CountVectorizer(ngram_range=ngram_range).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return words_freq[:top_k]

# Get top bi-grams for positive and negative tweets
top_bi_grams_positive = get_top_ngrams(df[df['label'] == 1]['text'], ngram_range=(2, 2), top_k=10)
top_bi_grams_negative = get_top_ngrams(df[df['label'] == 0]['text'], ngram_range=(2, 2), top_k=10)

# Display top bi-grams
print("Top Bi-grams in Positive Tweets:")
print(top_bi_grams_positive)

print("\nTop Bi-grams in Negative Tweets:")
print(top_bi_grams_negative)

